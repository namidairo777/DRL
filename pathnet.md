#PathNet: Evolution Channels Gradient Descent in Super Neural Networks
https://arxiv.org/pdf/1701.08734.pdf
## Abstract
- Area: artificial general intelligence (AGI)
- Neural network  parameter reuse without **catastrophic forgetting**!
- Reuse parameters in A and evolve a new path for B, which allows B to learn faster
- Support both supervised learning (MNIST, CIFAR, SVHN) and reinforcement learning (Atari, Labyrinth)
- Significant improvement on A3C (Asynchronous Advantage Actor-Critic)
## Keywords
Giant networks, Path evolution algorithm, Transfer Learning, Continual learning, Multi-Task learning.
## 1 Introduction
⋅⋅⋅To achieve that a network can reuse existing knowledge instead of learning from scratch for each task.
⋅⋅⋅They proposed that each user? of the giant net be given a population of agents whose job it is to learn the user-defined task as efficiently as possible.
⋅⋅⋅
